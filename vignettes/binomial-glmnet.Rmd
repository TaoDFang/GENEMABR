---
title: "Understanding binomial glmnet"
author: "Jitao David Zhang"
date: "`r format(Sys.Date(), '%B %d, %Y')`"
output: 
  BiocStyle::pdf_document:
    toc: true
vignette: >
    %\VignetteIndexEntry{understanding binomial glmnet}
    %\VignetteEngine{knitr::rmarkdown}
---

In this document, we compare elastic net results using the Gaussian family and the binomial family.

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE,
                      message=FALSE)
library(GENEMABR) 
library(glmnet)
```  


## Reguarlized linear regression

First, we use the Gaussian family, namely using the simplest linear regression.

```{r gaussianFit}
gene_list <- c("TRPC4AP","CDC37","TNIP1","IKBKB","NKIRAS2","NFKBIA","TIMM50","RELB",
           "TNFAIP3","NFKBIB","HSPA1A","NFKBIE","SPAG9",
           "NFKB2","ERLIN1","REL","TNIP2","TUBB6","MAP3K8")
set.seed(1887)
gaussRes <- regression_selected_pathways(gene_input=gene_list, alpha=0.5, family="gaussian")
gaussRes$selected_pathways_names
```
 

### Visualizaton of the feature-selection process of the linear model

```{r linearPlot}
plot(gaussRes$model)
```

Interestingly, we observe that the mean squared error (MSE) first decreases with increasing number of features (gene-sets in this case) selected, and then increase. This can be read from the plot below, from right to the left. This is apparently different from most prediction tasks, where MSE decreases with more features are used.

I believe that this is due to the dichotomy nature of both dependent and independent variables. With more and more gene-sets are used for prediction, there will be more false-negative results, namely genes are correctly predicted to be in the set of genes of interest (GOI). However more false-positive results are expected as well, namely genes outside the set of GOI are wrongly predicted to be members of GOI. Therefore, we use cross validation to select the optimal regularization parameter $\lambda$ in order to identify a minimal set of gene-sets that describe GOI.

### Identifying the leading-edge genes

By comparing the predicted `y` value (GOI membership) with the observed values, we notice that the prediction is far from perfect. Given the sparsity of the GOI membership (~1/1000), the model is supported by a few genes within GOI while many other genes within GOI are wrongly predicted to have the same response variable as genes out of GOI.

```{r gaussPredict}
gaussPred <- predict(gaussRes$model, newx=gaussRes$x, s="lambda.min")
table(predY=gaussPred, obsY=gaussRes$y)
```

Below are the genes that are supporting the model. They apparently are associated with NF-kB signalling pathway.


```{r gaussTruePosGenes}
## genes that drive the prediction using the Gaussian model
## they are apparently NF-kB relevant genes
rownames(gaussRes$x)[which(gaussPred>0.12 & gaussRes$y==1)]
```

And below are the genes that are likely false-positives, namely these genes are not within GOI, but the model predicts otherwise. Indeed these are also genes associated with the NF-kB pathway.

```{r gaussFalsePosGenes}
## genes that drive the prediction using the Gaussian model
## they are apparently NF-kB relevant genes 
rownames(gaussRes$x)[which(gaussPred>0.12 & gaussRes$y==0)]
```

## Reguarlized logistic regression

Instead of simple linear regression, we can also use logistic regression to model the dichotomy of the dependent variable. Though the concept is similar to the linear regression, we found that the solution is not stable. Running the model several times will give partially overlapping but different results. It is worth mentioning though that empirical observations suggest the results are similar to the linear-regression case.

```{r binomFit}
set.seed(1887)
binomRes <- regression_selected_pathways(gene_input=gene_list,family="binomial",alpha=0.5,
                                         type.measure="deviance")
length(binomRes$selected_pathways_names)
```

### Visualization of the feature-seletion process

```{r binomPlot
plot(binomRes$model)
```

The instability partially comes from the fact that the binomial deviance is not strictly convex but with at least two local minima (see example above). Several papers including the original paper discussed possible reasons and I believe that the observation deserves further study. For the current implementation, we stick to the simply linear regression for its simplicity and for its robustness.

### Prediction performance and leading-edge genes

The prediction results are quite similar with those of the linear model.

```{r binomPerformance}
binomPred <- predict(binomRes$model, newx=binomRes$x, s="lambda.min")
table(predY=binomPred, obsY=binomRes$y)
```

```{r binomTruePosGenes}
## genes that drive the prediction using the binomian model
## they are apparently NF-kB relevant genes
rownames(binomRes$x)[which(binomPred>(-3) & binomRes$y==1)]
```

And below are the genes that are likely false-positives, namely these genes are not within GOI, but the model predicts otherwise. Note that beyond the genes associated with the NF-kB pathway, other genes are identified as well.

```{r binomFalsePosGenes}
## genes that drive the prediction using the binomian model
## they are apparently NF-kB relevant genes 
rownames(binomRes$x)[which(binomPred>(-3) & binomRes$y==0)]
```

## Conclusions

In this document, we compare the results of regularized linear regression and those of regularized logistic regression. We note that the linear regression provides stable solutions that are comparable with the results of regularized logistic regression. Therefore, for the implementation of the `gerr` package, we use the cross-validation version of the elastic net using the  `gaussian` family (linear regression) as the default, though the `binomial` family can be specified by the user as well.

## R session info

```{r session, results='asis'}
sessionInfo()
```

